{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Fraudulent Activity through Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auditing is the practice of examining businesses' financial records compared to their financial statements to ensure they comply with standard accounting laws (Hooda et. al.). It is conducted primarily through three methods known as process auditing, which verifies the processes in a business conform to predetermined rules; product auditing, which examines if a specific product or service follows the necessary requirements; finally, system auditing that involves the analysis of a management system (“What Is Auditing?”). Furthermore, there are various audit categories. For instance, internal audits are conducted within the organization, while an external audit is completely independent of the company (“What Is Auditing?”). The selected dataset is an example of an external process audit. \n",
    "\n",
    "The dataset used will be Audit Data, which was collected by the Comptroller and Auditor General of India for the 2015-2016 year from the Auditor General Office of the CAG. The data was collected by an external audit company and was used previously to improve algorithms predicting the quality of auditing work. The set features information from 777 various firms across 46 different cities and 14 industrial sectors in India. The purpose of this data analysis project centers on the prevention of fraudulent firm activity. Data analysis and machine learning are promising methods of determining and predicting fraud activity due to the systematic workflow which has few variations across firms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/picture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1. Audit Work-flow from Hooda, Nishtha, et al. “Fraudulent Firm Classification: A Case Study of an External Audit.” Applied Artificial Intelligence, vol. 32, no. 1, 2018, pp. 48–64., https://doi.org/10.1080/08839514.2018.1451032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.3.0 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.2     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.0.3     \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.2\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.1.2     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 1.3.1     \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.0\n",
      "\n",
      "Warning message:\n",
      "“package ‘ggplot2’ was built under R version 4.0.1”\n",
      "Warning message:\n",
      "“package ‘tibble’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘tidyr’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘dplyr’ was built under R version 4.0.2”\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n",
      "Warning message:\n",
      "“package ‘tidymodels’ was built under R version 4.0.2”\n",
      "── \u001b[1mAttaching packages\u001b[22m ────────────────────────────────────── tidymodels 0.1.1 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mbroom    \u001b[39m 0.7.0      \u001b[32m✔\u001b[39m \u001b[34mrecipes  \u001b[39m 0.1.13\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdials    \u001b[39m 0.0.9      \u001b[32m✔\u001b[39m \u001b[34mrsample  \u001b[39m 0.0.7 \n",
      "\u001b[32m✔\u001b[39m \u001b[34minfer    \u001b[39m 0.5.4      \u001b[32m✔\u001b[39m \u001b[34mtune     \u001b[39m 0.1.1 \n",
      "\u001b[32m✔\u001b[39m \u001b[34mmodeldata\u001b[39m 0.0.2      \u001b[32m✔\u001b[39m \u001b[34mworkflows\u001b[39m 0.2.0 \n",
      "\u001b[32m✔\u001b[39m \u001b[34mparsnip  \u001b[39m 0.1.3      \u001b[32m✔\u001b[39m \u001b[34myardstick\u001b[39m 0.0.7 \n",
      "\n",
      "Warning message:\n",
      "“package ‘broom’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘dials’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘infer’ was built under R version 4.0.3”\n",
      "Warning message:\n",
      "“package ‘modeldata’ was built under R version 4.0.1”\n",
      "Warning message:\n",
      "“package ‘parsnip’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘recipes’ was built under R version 4.0.1”\n",
      "Warning message:\n",
      "“package ‘tune’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘workflows’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘yardstick’ was built under R version 4.0.2”\n",
      "── \u001b[1mConflicts\u001b[22m ───────────────────────────────────────── tidymodels_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mscales\u001b[39m::\u001b[32mdiscard()\u001b[39m masks \u001b[34mpurrr\u001b[39m::discard()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m   masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mfixed()\u001b[39m  masks \u001b[34mstringr\u001b[39m::fixed()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m      masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31m✖\u001b[39m \u001b[34myardstick\u001b[39m::\u001b[32mspec()\u001b[39m masks \u001b[34mreadr\u001b[39m::spec()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mstep()\u001b[39m   masks \u001b[34mstats\u001b[39m::step()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Run this cell before continuing.\n",
    "library(tidyverse)\n",
    "library(tidymodels)\n",
    "library(repr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After a preliminary investigation into the paper utilizing this data we determined the most important factors were Score A, Score B, Money Value, Total, Loss, and History. However, to improve the final data visualization of the project we narrowed it down to two factors:  the amount of money involved in misstatements in past audits (Money_Value), and the total amount of discrepancy found in past audit reports (TOTAL), meaning that this two would be our variables and the Risk column is what we are looking to predict . We began by downloading and loading the final trial.csv  in our GitHub, which is the data found on the Audit Data website. We have decided to use Github as it is a group project and this way everyone has access to the data and can see the latest changes. When loading the data, we started unwrganling and cleaning the data by  mutating and removing all the unnecessary parts. We then were able to get a clear visualization of our first table. From there we moved on to the classification part of it. We split the data to our training and testing sets so we can use our classifier to later make predictions on on what would be our new data, and specified we wanted to use 75% of the given data, created a recipe, scaled our predictors (to make sure they were as suitable as possible) and centered them. We decided to use cross-validation which divides the data into equal chunks and compares them to make sure to get the best accuracy. This lets us choose our k that gives us the lowest RMSPE , so we created a workflow, which was composed by our recipe and mode specification. After collecting the metrics we were able to visualize our graph which plotted k vs the accuracy to be able to choose the best k value, in this case six.  With this information we were able to build our final model, only this time we used the fit model. Using this data we were able to predict from our chosen database as well as analyze the accuracy of our predictions which had 88.6% of accuracy, and added a confusion matrix which exposed that 7 Indian non-fraudulent companies are predicted fraudulent and 15 Indian fraudulent companies are predicted otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the file `trial.csv` from the `audit_data` folder in our GitHub repo. We `select()` the `Money_Value` and `TOTAL` as the variables we want to use, and the `Risk` column as the variable we try to predict. Then, we `mutate()` the `Risk` column such that it is a factor using `as_factor()`, and assign our data to an object called `audit_trial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed with column specification:\n",
      "cols(\n",
      "  Sector_score = \u001b[32mcol_double()\u001b[39m,\n",
      "  LOCATION_ID = \u001b[31mcol_character()\u001b[39m,\n",
      "  PARA_A = \u001b[32mcol_double()\u001b[39m,\n",
      "  SCORE_A = \u001b[32mcol_double()\u001b[39m,\n",
      "  PARA_B = \u001b[32mcol_double()\u001b[39m,\n",
      "  SCORE_B = \u001b[32mcol_double()\u001b[39m,\n",
      "  TOTAL = \u001b[32mcol_double()\u001b[39m,\n",
      "  numbers = \u001b[32mcol_double()\u001b[39m,\n",
      "  Marks = \u001b[32mcol_double()\u001b[39m,\n",
      "  Money_Value = \u001b[32mcol_double()\u001b[39m,\n",
      "  MONEY_Marks = \u001b[32mcol_double()\u001b[39m,\n",
      "  District = \u001b[32mcol_double()\u001b[39m,\n",
      "  Loss = \u001b[32mcol_double()\u001b[39m,\n",
      "  LOSS_SCORE = \u001b[32mcol_double()\u001b[39m,\n",
      "  History = \u001b[32mcol_double()\u001b[39m,\n",
      "  History_score = \u001b[32mcol_double()\u001b[39m,\n",
      "  Score = \u001b[32mcol_double()\u001b[39m,\n",
      "  Risk = \u001b[32mcol_double()\u001b[39m\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audit_trial <- read_csv(\"audit_data/trial.csv\") %>%\n",
    "    select(Money_Value, TOTAL, Risk) %>%\n",
    "    mutate(Risk = as_factor(Risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we check and remove the rows with missing values using `is.na()` in `subset()` fuction and `na.omit()`fuction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 1 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Money_Value</th><th scope=col>TOTAL</th><th scope=col>Risk</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>NA</td><td>0.23</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 3\n",
       "\\begin{tabular}{lll}\n",
       " Money\\_Value & TOTAL & Risk\\\\\n",
       " <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t NA & 0.23 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 3\n",
       "\n",
       "| Money_Value &lt;dbl&gt; | TOTAL &lt;dbl&gt; | Risk &lt;fct&gt; |\n",
       "|---|---|---|\n",
       "| NA | 0.23 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  Money_Value TOTAL Risk\n",
       "1 NA          0.23  0   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 775 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Money_Value</th><th scope=col>TOTAL</th><th scope=col>Risk</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>  3.380</td><td> 6.68</td><td>1</td></tr>\n",
       "\t<tr><td>  0.940</td><td> 4.83</td><td>0</td></tr>\n",
       "\t<tr><td>  0.000</td><td> 0.74</td><td>0</td></tr>\n",
       "\t<tr><td> 11.750</td><td>10.80</td><td>1</td></tr>\n",
       "\t<tr><td>  0.000</td><td> 0.08</td><td>0</td></tr>\n",
       "\t<tr><td>  2.950</td><td> 0.83</td><td>0</td></tr>\n",
       "\t<tr><td> 44.950</td><td> 8.51</td><td>1</td></tr>\n",
       "\t<tr><td>  7.790</td><td>20.53</td><td>1</td></tr>\n",
       "\t<tr><td>  7.340</td><td>19.45</td><td>1</td></tr>\n",
       "\t<tr><td>  1.930</td><td> 4.97</td><td>1</td></tr>\n",
       "\t<tr><td>  4.420</td><td>16.20</td><td>1</td></tr>\n",
       "\t<tr><td>  0.960</td><td>55.52</td><td>1</td></tr>\n",
       "\t<tr><td> 10.430</td><td>13.10</td><td>1</td></tr>\n",
       "\t<tr><td>  0.000</td><td> 1.44</td><td>1</td></tr>\n",
       "\t<tr><td>  0.007</td><td> 0.84</td><td>0</td></tr>\n",
       "\t<tr><td>  9.000</td><td>10.96</td><td>1</td></tr>\n",
       "\t<tr><td> 41.280</td><td>40.17</td><td>1</td></tr>\n",
       "\t<tr><td> 14.030</td><td> 9.01</td><td>1</td></tr>\n",
       "\t<tr><td>  0.000</td><td> 2.84</td><td>1</td></tr>\n",
       "\t<tr><td> 63.180</td><td>51.64</td><td>1</td></tr>\n",
       "\t<tr><td> 34.240</td><td>20.36</td><td>1</td></tr>\n",
       "\t<tr><td>  0.010</td><td> 5.96</td><td>1</td></tr>\n",
       "\t<tr><td>205.190</td><td>28.10</td><td>1</td></tr>\n",
       "\t<tr><td>  0.100</td><td> 0.95</td><td>0</td></tr>\n",
       "\t<tr><td> 11.160</td><td>63.70</td><td>1</td></tr>\n",
       "\t<tr><td>  1.250</td><td> 9.66</td><td>1</td></tr>\n",
       "\t<tr><td>  0.007</td><td> 1.10</td><td>0</td></tr>\n",
       "\t<tr><td>  1.460</td><td>38.61</td><td>1</td></tr>\n",
       "\t<tr><td>  0.000</td><td> 1.03</td><td>0</td></tr>\n",
       "\t<tr><td>  6.780</td><td> 0.75</td><td>1</td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td>0.00</td><td>0.2517</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>0.3115</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>0.0000</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>0.8400</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>1.0900</td><td>1</td></tr>\n",
       "\t<tr><td>0.00</td><td>1.2900</td><td>1</td></tr>\n",
       "\t<tr><td>0.00</td><td>0.8800</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>0.0900</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>1.3000</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>1.0700</td><td>1</td></tr>\n",
       "\t<tr><td>0.00</td><td>0.0000</td><td>0</td></tr>\n",
       "\t<tr><td>0.18</td><td>3.4700</td><td>1</td></tr>\n",
       "\t<tr><td>0.00</td><td>1.0400</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>1.4900</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>2.0100</td><td>1</td></tr>\n",
       "\t<tr><td>0.00</td><td>0.0000</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>0.5900</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>0.0200</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>5.9600</td><td>1</td></tr>\n",
       "\t<tr><td>0.00</td><td>3.3000</td><td>1</td></tr>\n",
       "\t<tr><td>0.00</td><td>1.3700</td><td>0</td></tr>\n",
       "\t<tr><td>0.21</td><td>0.9000</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>0.9700</td><td>0</td></tr>\n",
       "\t<tr><td>0.09</td><td>1.0100</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>1.2000</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>0.8900</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>0.8400</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>0.2800</td><td>0</td></tr>\n",
       "\t<tr><td>0.00</td><td>0.2000</td><td>0</td></tr>\n",
       "\t<tr><td>0.32</td><td>0.0000</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 775 × 3\n",
       "\\begin{tabular}{lll}\n",
       " Money\\_Value & TOTAL & Risk\\\\\n",
       " <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t   3.380 &  6.68 & 1\\\\\n",
       "\t   0.940 &  4.83 & 0\\\\\n",
       "\t   0.000 &  0.74 & 0\\\\\n",
       "\t  11.750 & 10.80 & 1\\\\\n",
       "\t   0.000 &  0.08 & 0\\\\\n",
       "\t   2.950 &  0.83 & 0\\\\\n",
       "\t  44.950 &  8.51 & 1\\\\\n",
       "\t   7.790 & 20.53 & 1\\\\\n",
       "\t   7.340 & 19.45 & 1\\\\\n",
       "\t   1.930 &  4.97 & 1\\\\\n",
       "\t   4.420 & 16.20 & 1\\\\\n",
       "\t   0.960 & 55.52 & 1\\\\\n",
       "\t  10.430 & 13.10 & 1\\\\\n",
       "\t   0.000 &  1.44 & 1\\\\\n",
       "\t   0.007 &  0.84 & 0\\\\\n",
       "\t   9.000 & 10.96 & 1\\\\\n",
       "\t  41.280 & 40.17 & 1\\\\\n",
       "\t  14.030 &  9.01 & 1\\\\\n",
       "\t   0.000 &  2.84 & 1\\\\\n",
       "\t  63.180 & 51.64 & 1\\\\\n",
       "\t  34.240 & 20.36 & 1\\\\\n",
       "\t   0.010 &  5.96 & 1\\\\\n",
       "\t 205.190 & 28.10 & 1\\\\\n",
       "\t   0.100 &  0.95 & 0\\\\\n",
       "\t  11.160 & 63.70 & 1\\\\\n",
       "\t   1.250 &  9.66 & 1\\\\\n",
       "\t   0.007 &  1.10 & 0\\\\\n",
       "\t   1.460 & 38.61 & 1\\\\\n",
       "\t   0.000 &  1.03 & 0\\\\\n",
       "\t   6.780 &  0.75 & 1\\\\\n",
       "\t ⋮ & ⋮ & ⋮\\\\\n",
       "\t 0.00 & 0.2517 & 0\\\\\n",
       "\t 0.00 & 0.3115 & 0\\\\\n",
       "\t 0.00 & 0.0000 & 0\\\\\n",
       "\t 0.00 & 0.8400 & 0\\\\\n",
       "\t 0.00 & 1.0900 & 1\\\\\n",
       "\t 0.00 & 1.2900 & 1\\\\\n",
       "\t 0.00 & 0.8800 & 0\\\\\n",
       "\t 0.00 & 0.0900 & 0\\\\\n",
       "\t 0.00 & 1.3000 & 0\\\\\n",
       "\t 0.00 & 1.0700 & 1\\\\\n",
       "\t 0.00 & 0.0000 & 0\\\\\n",
       "\t 0.18 & 3.4700 & 1\\\\\n",
       "\t 0.00 & 1.0400 & 0\\\\\n",
       "\t 0.00 & 1.4900 & 0\\\\\n",
       "\t 0.00 & 2.0100 & 1\\\\\n",
       "\t 0.00 & 0.0000 & 0\\\\\n",
       "\t 0.00 & 0.5900 & 0\\\\\n",
       "\t 0.00 & 0.0200 & 0\\\\\n",
       "\t 0.00 & 5.9600 & 1\\\\\n",
       "\t 0.00 & 3.3000 & 1\\\\\n",
       "\t 0.00 & 1.3700 & 0\\\\\n",
       "\t 0.21 & 0.9000 & 0\\\\\n",
       "\t 0.00 & 0.9700 & 0\\\\\n",
       "\t 0.09 & 1.0100 & 0\\\\\n",
       "\t 0.00 & 1.2000 & 0\\\\\n",
       "\t 0.00 & 0.8900 & 0\\\\\n",
       "\t 0.00 & 0.8400 & 0\\\\\n",
       "\t 0.00 & 0.2800 & 0\\\\\n",
       "\t 0.00 & 0.2000 & 0\\\\\n",
       "\t 0.32 & 0.0000 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 775 × 3\n",
       "\n",
       "| Money_Value &lt;dbl&gt; | TOTAL &lt;dbl&gt; | Risk &lt;fct&gt; |\n",
       "|---|---|---|\n",
       "|   3.380 |  6.68 | 1 |\n",
       "|   0.940 |  4.83 | 0 |\n",
       "|   0.000 |  0.74 | 0 |\n",
       "|  11.750 | 10.80 | 1 |\n",
       "|   0.000 |  0.08 | 0 |\n",
       "|   2.950 |  0.83 | 0 |\n",
       "|  44.950 |  8.51 | 1 |\n",
       "|   7.790 | 20.53 | 1 |\n",
       "|   7.340 | 19.45 | 1 |\n",
       "|   1.930 |  4.97 | 1 |\n",
       "|   4.420 | 16.20 | 1 |\n",
       "|   0.960 | 55.52 | 1 |\n",
       "|  10.430 | 13.10 | 1 |\n",
       "|   0.000 |  1.44 | 1 |\n",
       "|   0.007 |  0.84 | 0 |\n",
       "|   9.000 | 10.96 | 1 |\n",
       "|  41.280 | 40.17 | 1 |\n",
       "|  14.030 |  9.01 | 1 |\n",
       "|   0.000 |  2.84 | 1 |\n",
       "|  63.180 | 51.64 | 1 |\n",
       "|  34.240 | 20.36 | 1 |\n",
       "|   0.010 |  5.96 | 1 |\n",
       "| 205.190 | 28.10 | 1 |\n",
       "|   0.100 |  0.95 | 0 |\n",
       "|  11.160 | 63.70 | 1 |\n",
       "|   1.250 |  9.66 | 1 |\n",
       "|   0.007 |  1.10 | 0 |\n",
       "|   1.460 | 38.61 | 1 |\n",
       "|   0.000 |  1.03 | 0 |\n",
       "|   6.780 |  0.75 | 1 |\n",
       "| ⋮ | ⋮ | ⋮ |\n",
       "| 0.00 | 0.2517 | 0 |\n",
       "| 0.00 | 0.3115 | 0 |\n",
       "| 0.00 | 0.0000 | 0 |\n",
       "| 0.00 | 0.8400 | 0 |\n",
       "| 0.00 | 1.0900 | 1 |\n",
       "| 0.00 | 1.2900 | 1 |\n",
       "| 0.00 | 0.8800 | 0 |\n",
       "| 0.00 | 0.0900 | 0 |\n",
       "| 0.00 | 1.3000 | 0 |\n",
       "| 0.00 | 1.0700 | 1 |\n",
       "| 0.00 | 0.0000 | 0 |\n",
       "| 0.18 | 3.4700 | 1 |\n",
       "| 0.00 | 1.0400 | 0 |\n",
       "| 0.00 | 1.4900 | 0 |\n",
       "| 0.00 | 2.0100 | 1 |\n",
       "| 0.00 | 0.0000 | 0 |\n",
       "| 0.00 | 0.5900 | 0 |\n",
       "| 0.00 | 0.0200 | 0 |\n",
       "| 0.00 | 5.9600 | 1 |\n",
       "| 0.00 | 3.3000 | 1 |\n",
       "| 0.00 | 1.3700 | 0 |\n",
       "| 0.21 | 0.9000 | 0 |\n",
       "| 0.00 | 0.9700 | 0 |\n",
       "| 0.09 | 1.0100 | 0 |\n",
       "| 0.00 | 1.2000 | 0 |\n",
       "| 0.00 | 0.8900 | 0 |\n",
       "| 0.00 | 0.8400 | 0 |\n",
       "| 0.00 | 0.2800 | 0 |\n",
       "| 0.00 | 0.2000 | 0 |\n",
       "| 0.32 | 0.0000 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "    Money_Value TOTAL  Risk\n",
       "1     3.380      6.68  1   \n",
       "2     0.940      4.83  0   \n",
       "3     0.000      0.74  0   \n",
       "4    11.750     10.80  1   \n",
       "5     0.000      0.08  0   \n",
       "6     2.950      0.83  0   \n",
       "7    44.950      8.51  1   \n",
       "8     7.790     20.53  1   \n",
       "9     7.340     19.45  1   \n",
       "10    1.930      4.97  1   \n",
       "11    4.420     16.20  1   \n",
       "12    0.960     55.52  1   \n",
       "13   10.430     13.10  1   \n",
       "14    0.000      1.44  1   \n",
       "15    0.007      0.84  0   \n",
       "16    9.000     10.96  1   \n",
       "17   41.280     40.17  1   \n",
       "18   14.030      9.01  1   \n",
       "19    0.000      2.84  1   \n",
       "20   63.180     51.64  1   \n",
       "21   34.240     20.36  1   \n",
       "22    0.010      5.96  1   \n",
       "23  205.190     28.10  1   \n",
       "24    0.100      0.95  0   \n",
       "25   11.160     63.70  1   \n",
       "26    1.250      9.66  1   \n",
       "27    0.007      1.10  0   \n",
       "28    1.460     38.61  1   \n",
       "29    0.000      1.03  0   \n",
       "30    6.780      0.75  1   \n",
       "⋮   ⋮           ⋮      ⋮   \n",
       "746 0.00        0.2517 0   \n",
       "747 0.00        0.3115 0   \n",
       "748 0.00        0.0000 0   \n",
       "749 0.00        0.8400 0   \n",
       "750 0.00        1.0900 1   \n",
       "751 0.00        1.2900 1   \n",
       "752 0.00        0.8800 0   \n",
       "753 0.00        0.0900 0   \n",
       "754 0.00        1.3000 0   \n",
       "755 0.00        1.0700 1   \n",
       "756 0.00        0.0000 0   \n",
       "757 0.18        3.4700 1   \n",
       "758 0.00        1.0400 0   \n",
       "759 0.00        1.4900 0   \n",
       "760 0.00        2.0100 1   \n",
       "761 0.00        0.0000 0   \n",
       "762 0.00        0.5900 0   \n",
       "763 0.00        0.0200 0   \n",
       "764 0.00        5.9600 1   \n",
       "765 0.00        3.3000 1   \n",
       "766 0.00        1.3700 0   \n",
       "767 0.21        0.9000 0   \n",
       "768 0.00        0.9700 0   \n",
       "769 0.09        1.0100 0   \n",
       "770 0.00        1.2000 0   \n",
       "771 0.00        0.8900 0   \n",
       "772 0.00        0.8400 0   \n",
       "773 0.00        0.2800 0   \n",
       "774 0.00        0.2000 0   \n",
       "775 0.32        0.0000 0   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audit_trial_missing <- subset(audit_trial, is.na(TOTAL) | is.na(Money_Value))\n",
    "audit_trial_missing\n",
    "\n",
    "audit_trial_clean <- na.omit(audit_trial)\n",
    "audit_trial_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning our data, our next step is to create the training and test set. First use the `initial_split` function to split `audit_trial_clean`. Specify we want to use *75%* of the data. For the `strata` argument, place the variable we want to classify, `Risk`. Name the object we create `audit_split`. Next, pass the `audit_split` object to the `training` and `testing` functions and name your respective objects as `audit_training` and `audit_testing`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed. Don't remove this!\n",
    "set.seed(9999)\n",
    "\n",
    "audit_split <- initial_split(audit_trial_clean, prop = 0.75, strata = Risk)\n",
    "audit_training <- training(audit_split)\n",
    "audit_testing <- testing(audit_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we want to see if `Money_Value` and `TOTAL` can predict `Risk`, so now we need to create a recipe for these variables. First, pass the vector `audit_training` and the predictors to the `recipe` function. To scale our predictors, use the `step_scale(all_predictors())` function. To center our predictors, use the `step_center(all_predictors())` function. We assign our recipe to an object called `audit_recipe`. \n",
    "\n",
    "Now, we will use cross-validation on the training data set to select which $k$ is the most optimal for our data set for k-nn classification. To be specific, we will:\n",
    "- Perform a 5-fold cross-validation on the training set, create a workflow analysis with our recipe and model specification and specify that the tuning should try 10 values of $K$\n",
    "- Collect the metrics from the workflow analysis \n",
    "- Plot the $K$ vs the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data Recipe\n",
       "\n",
       "Inputs:\n",
       "\n",
       "      role #variables\n",
       "   outcome          1\n",
       " predictor          2\n",
       "\n",
       "Operations:\n",
       "\n",
       "Scaling for all_predictors()\n",
       "Centering for all_predictors()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "K-Nearest Neighbor Model Specification (classification)\n",
       "\n",
       "Main Arguments:\n",
       "  neighbors = tune()\n",
       "  weight_func = rectangular\n",
       "\n",
       "Computational engine: kknn \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAADAFBMVEUAAAABAQECAgIDAwME\nBAQFBQUGBgYHBwcICAgJCQkKCgoLCwsMDAwNDQ0ODg4PDw8QEBARERESEhITExMUFBQVFRUW\nFhYXFxcYGBgZGRkaGhobGxscHBwdHR0eHh4fHx8gICAhISEiIiIjIyMkJCQlJSUmJiYnJyco\nKCgpKSkqKiorKyssLCwtLS0uLi4vLy8wMDAxMTEyMjIzMzM0NDQ1NTU2NjY3Nzc4ODg5OTk6\nOjo7Ozs8PDw9PT0+Pj4/Pz9AQEBBQUFCQkJDQ0NERERFRUVGRkZHR0dISEhJSUlKSkpLS0tM\nTExNTU1OTk5PT09QUFBRUVFSUlJTU1NUVFRVVVVWVlZXV1dYWFhZWVlaWlpbW1tcXFxdXV1e\nXl5fX19gYGBhYWFiYmJjY2NkZGRlZWVmZmZnZ2doaGhpaWlqampra2tsbGxtbW1ubm5vb29w\ncHBxcXFycnJzc3N0dHR1dXV2dnZ3d3d4eHh5eXl6enp7e3t8fHx9fX1+fn5/f3+AgICBgYGC\ngoKDg4OEhISFhYWGhoaHh4eIiIiJiYmKioqLi4uMjIyNjY2Ojo6Pj4+QkJCRkZGSkpKTk5OU\nlJSVlZWWlpaXl5eYmJiZmZmampqbm5ucnJydnZ2enp6fn5+goKChoaGioqKjo6OkpKSlpaWm\npqanp6eoqKipqamqqqqrq6usrKytra2urq6vr6+wsLCxsbGysrKzs7O0tLS1tbW2tra3t7e4\nuLi5ubm6urq7u7u8vLy9vb2+vr6/v7/AwMDBwcHCwsLDw8PExMTFxcXGxsbHx8fIyMjJycnK\nysrLy8vMzMzNzc3Ozs7Pz8/Q0NDR0dHS0tLT09PU1NTV1dXW1tbX19fY2NjZ2dna2trb29vc\n3Nzd3d3e3t7f39/g4ODh4eHi4uLj4+Pk5OTl5eXm5ubn5+fo6Ojp6enq6urr6+vs7Ozt7e3u\n7u7v7+/w8PDx8fHy8vLz8/P09PT19fX29vb39/f4+Pj5+fn6+vr7+/v8/Pz9/f3+/v7////i\nsF19AAAACXBIWXMAABJ0AAASdAHeZh94AAAgAElEQVR4nO3deWAU5fnA8ReQQw5RpOJF0WqL\n9axF8UC0VcvPtgZRBBQkioptxasq3lSpBVotxYrailLbile9ES9EW0VFxQPFCxVEuRmu3Mcm\neX/zPJNNdpPMuzvjIgG+3z+S2X3zTMLsfshms9k1loi+cWZTfwFEW0JAIspBQCLKQUAiykFA\nIspBQCLKQUAiykFAIspBuYJUvJ35SY52FaP3zF7BxoOtun2y6b4M2nrLFaQ7zffMpznaV/SS\nkB7bZqePNtkXQVtzuYL0o05PmktytK/o1UKa0abr/E32NdBWXY4gvW6GV+2yQ1lwovr2Qzp0\nPPbl9M1R5h45Y475pbXXmCdv37WztQVX7dOuzd6XF6R9ZB8zQ/fyX9Mn2N0xZrq+f8oca+3D\nP92h9S4nPNPg0weQZrbd8f3c/HOIIpYjSMPNLHuF+Vdw4lSz7/nDOpp/p22mQrrBXNr+9JG2\nsq/pdfnFPc2hVakfebcZqHv5jfl7sLs7zZm1n+MeO8V851djzu7S4t/pn14h/XfbLu/l5l9D\nFLXcQFrTbo8a+6k5Qk88YH7uy/i0fYei1M1USONN5+f97UfN4f5qxT7yHaf+Iwvbt1njL1bt\n1HZ9sPN1bXao9N+Vd9620B5gvvA3l3Q6PP3zC6RXO+7wTk7+MUTRyw2km8xY/+2RRm9Z9TOz\n5d2kyxambqZCmmAUwpePzZV3V5o/pA0NN7f6Wy+YU5N7zzPP+W+fMKdZ273FCjmnosHn9yG9\ntZ2ZlpN/C1GMcgKpZq+WX/nv7jbny6kOpjS5kLKZDum3ybMLV6wYa65N+8iXzI/9tyPNk8kz\nHjDn+m+HmqetPd/sM3VF4y/gPdN5h86mx8pc/GOIYpQTSM+afvKuqMN2xdYWm3bJ81M2G0Aa\nr+c93qedka5N+8iaPc0HNrFj18rkGSUdu1bZsk47Jayt/FVrY/a9YlGDL+A9Y45Zfok5tNQS\nbZJyAqm/STbF2lLTqqb2/JTNBpBulu07TadL7n/62V/5kFI/0l5vLvVtXli//2HmRf8Hqot0\ne/nfT+pk2jyU/gW8Z3avslW/MCdX5+KfQxS5XED6utX2Z2qnml7+yU7GS66kbF5g/iHvHkuB\ntJvRu8h/JzftUj7Sftlit5rhZm79J3jav814Wv0ZZX/bZvvytK8guPu7YD9fINGmKBeQrq37\nkWcfubb/1Dwg2+OPey1183IzUTavrodUbjrKOTW9BVLKR8qJZzr+MOUTJLp2L+2wj2wtXq5n\nHG0+TvsKan8hu6ir+VsO/j1EkcsBpMqdTfJxOTebc6z9p+nl/6j05Q7t16Vu3mGO8W+8fdw1\n5TtSF/O17+iGnc2otCFr/2X2rP0pqrbfmHFy156dZ46VO+wKd2+1xtqpd5Uk15MPEXqlTatn\nv/k/iChyOYD0kDkqubm6TYcNtvpE0+PXwzuZu2zq5qrtzBG/Pa3jzebndZAuNd//wx8O7/m8\n2fGPS1I+0tqSTqbl16mfYrbp1OJL2Rhqvjfqd+f3MBf7263MkuR63YNW/2E68eAG2gTlANJP\nko9o8BtkJvu3xCYeuG2Ho1+S0ymb849t3/GwJzx5kHgtpLJr92rb/fw19qwOO3+Q+pHWniOP\nBkqpZg/TVzeqbz+ya6vOff8h90w0Bcm/Bbn7sm/+TyKKWPP8e6Tx5t5N/SUQRalZQqrs3rU8\n80cRNZ+aJaRLzO839ZdAFKnmB+mTK48yB/EQBdq8an6QXmzZ8fTVm/qLIIpW84NEtBkGJKIc\nBCSiHAQkohwEJKIcBCSiHAQkohwEJKIcBCSiHJQDSMUb/IpLNoRXWelYLHFNJioci6XFrsly\nx2JZUfhaQaLMMVleGL5W6JysKAhfK0qUuiYda8UJ54F3TroOn/MSS7gOn/MSS7gOn/MSc046\nL7GE68A7L7GEY7FYL7HCXELa4PkVlXjhVVc7FkuKHIs1CcdiaWH42hpb6Zgs2xC+ttaWOyYr\n1oevrbdljsnKteFrG6zr8FWtCV8rtMWOSddxL7KuA++8xGxB+OIa5yVmHQd+nesSK7frwhfX\nOy8x6zjwBa5LLGEdi4V6ia0DUmhAyjQJJA9IQUACkgYkCUhA0oAkAQlIGpBkj0ACkgYkIAUB\nKdMkkDwgBQEJSBqQJCABSQOSBCQgaUCSPQIJSBqQgBQEpEyTQPKAFAQkIGlAkoAEJA1IEpCA\npAFJ9ggkIGlAAlIQkDJNAskDUhCQgKQBSQISkDQgSUACkgYk2SOQgKQBCUhBQMo0CSQPSEFA\nApIGJAlIQNKAJAEJSBqQZI9AApIGJCAFASnTJJA8IAUBCUjatwhp6WUnJTeLJp45dOyq+vdA\nSg9IHpDCeiV/Uh2kG69ctOzmUdV174GUHpA8IIX14uo5SUhe/4X+d6MB85LvgdQgIHlACq8O\n0usDa/y3FzyUfA+kBgHJA1IWkJ47S95eOyX53n+z8Fa/L0r9KipLw6upcSxWVjgWbbVjMRF/\nsjx8rcxWOSarHJPlNuGYrC6LO+k6ehXWeeAda5XWdfjiTzqPu3UdeNdklXUdPucl5px0XmLW\nsVh7hY8FaUQtpBF1kF7q5fdmxl0QbZnV3VEQBdIbwU26h5Pv/Tdr3/RbLq8BWOp63cLqatdL\nE7pe9bGmyjXpeNXHAut63cIKx6s+FlrXyz66XvWxyLpe9tH12o3F1nn4HK/dWGJdL95Y41gr\ntc4D71grs64D77zErOvwOS8x6zh8Rc5LzLoOn+sSq7KOxRK9xKK89GUdpLX9P7e24KQPk++T\nH8DPSLXxM5LHz0hhrfNmnuRf8ezM6dZOuGTR0hsural7D6T0gOQBKaxz8qQn7U3XWVsyKX/Y\n+HX174GUHpA8IH2DgFQbkDwgASkZkDJNAskDUhCQgKQBSQISkDQgSUACkgYk2SOQgKQBCUhB\nQMo0CSQPSEFAApIGJAlIQNKAJAEJSBqQZI9AApIGJCAFASnTJJA8IAUBCUgakCQgAUkDkgQk\nIGlAkj0CCUgakIAUBKRMk0DygBQEJCBpQJKABCQNSBKQgKQBSfYIJCBpQAJSEJAyTQLJA1IQ\nkICkAUkCEpA0IElAApIGJNkjkICkAQlIQUDKNAkkD0hBQAKSBiQJSEDSgCQBCUgakGSPQAKS\nBiQgBQEp0ySQPCAFAQlIGpAkIAFJA5IEJCBpQJI9AglIGpCAFASkTJNA8oAUBCQgaUCSgAQk\nDUgSkICkAUn2CCQgaUACUhCQMk0CyQNSEJCApAFJAhKQNCBJQAKSBiTZI5CApAEJSEFAyjQJ\nJA9IQUACkgYkCUhA0oAkAQlIGpBkj0ACkgYkIAUBKdMkkDwgBQEJSBqQJCABSQOSBCQgaUCS\nPQIJSBqQgBQEpEyTQPKAFAQkIGlAkoAEJA1IEpCApAFJ9ggkIGlAAlIQkDJNAskDUhCQgKQB\nSQISkDQgSUACkgYk2SOQgKQBCUhBQMo0CSQPSEFAApIGJAlIQNKAJAEJSBqQZI9AApIGJCAF\nASnTJJA8IAUBCUgakCQgAUkDkgQkIGlAkj0CCUgakIAUBKRMk0DygBQEJCBpQJKABCQNSBKQ\ngKRt9ZAqq/yqq6vCs9ax6J6scSzWNLPJavekY8096Tx61vlP2QSTzmPwDSZdX9Amm0zkElLB\nGr/ikjXhVVc7FkuKHYs1VY7F0sLwtbW20jFZXhC+ts5WOCYrNoSv+d+RHJOJdeFrBbbUMVm1\nNnyt0DoPvGOt2BbFnCyxrgPvusTKrOvwOS8xuz58cYPzErOuw+e8xKxjsUgvsfW5hMRNu9q4\naedx0w5IyYCUaRJIHpCCgAQkDUgSkICkAUkCEpA0IMkegQQkDUhACgJSpkkgeUAKAhKQNCBJ\nQAKSBiQJSEDSgCR7BBKQNCABKQhImSaB5AEpCEhA0oAkAQlIGpAkIAFJA5LsEUhA0oAEpCAg\nZZoEkgekICABSQOSBCQgaUCSgAQkDUiyRyABSQMSkIKAlGkSSB6QgoAEJA1IEpCApAFJAhKQ\nNCDJHoEEJA1IQAoCUqZJIHlACgISkDQgSUACkgYkCUhA0oAkewQSkDQgASkISJkmgeQBKQhI\nQNKAJAEJSBqQJCABSQOS7BFIQNKABKQgIGWaBJIHpCAgAUkDkgQkIGlAkoAEJA1IskcgAUkD\nEpCCgJRpEkgekIKABCQNSBKQgKQBSQISkDQgyR6BBCQNSEAKAlKmSSB5QAoCEpA0IElAApIG\nJAlIQNKAJHsEEpA0IAEpCEiZJoHkASkISEDSgCQBCUgakCQgAUkDkuwRSEDSgASkICBlmgSS\nB6QgIAFJA5IEJCBpQJKABCQNSLJHIAFJAxKQgoCUaRJIHpCCgAQkDUgSkICkAUkCEpA0IMke\ngQQkDUhACgJSpkkgeUAKAhKQNCBJQAKSBiQJSEDSgCR7BBKQNCABKQhImSaB5AEpCEhA0oAk\nAQlIGpAkIAFJA5LsEUhA0oAEpCAgZZoEkgekICABSQOSBCQgaZsDpKKJZw4duyrYXjH+jEF/\n3GDthXl+g4DUMCB5QArrxisXLbt5VLVsVv7qxqWLr7va2hFPeXJ9A1KDgOQBKSSv/0L/u9KA\nebK9IG+Nf0beYnvq3LSPAVJtQPKAFNLrA2v8txc8JNsf5hVaWzVgVmXerRefPX6pnJUo8Fu3\nxq+4dE141dWOxdJix2JNlWOxrCh8ba2tdEyWF4SvrbMVjsmKDeFrPiTHZOW68LUC6zp8VWvD\n1wptiWPSddyLrePwOSdLbGH44tqEY7LMug6f8xKz68MXN5Q7Jn1I4YuFrsmEdSwW6SW2PltI\nz50lb6+dIm9Lh/09kbhvwOMbhv9lwYIbhhf7Z73Uy+9N9y6Ittiq67YyQRohbwNIdv55A4bc\nd9503S4dNNN/O+83fvMr/aqqKsOrqXEsOieta7I6/mQifC1hqzfKpOvriT9ZZZ0HvplNVlvH\n4Ys/mXAePetYjD8ZXG0rsoX0RnDT7uHak8WJxMlzgs3z709+DD8j1cbPSB4/I4W0tv/n1hac\n9KFsV73ij701oGDx5IS1ZYNeAlKDgOQBKawJlyxaesOlNXamf4vu4vHeh2feYQuHTlqxdPyI\nciA1CEgekMIqmZQ/bLz/4TddZ+2ya0494y7/u9HC64accePKug8BUm1A8oD0DQJSbUDygASk\nZEDKNAkkD0hBQAKSBiQJSEDSgCQBCUgakGSPQAKSBiQgBQEp0ySQPCAFAQlIGpAkIAFJA5IE\nJCBpQJI9AglIGpCAFASkTJNA8oAUBCQgaUCSgAQkDUgSkICkAUn2CCQgaUACUhCQMk0CyQNS\nEJCApAFJAhKQNCBJQAKSBiTZI5CApAEJSEFAyjQJJA9IQUACkgYkCUhA0oAkAQlIGpBkj0AC\nkgYkIAUBKdMkkDwgBQEJSBqQJCABSQOSBCQgaUCSPQIJSBqQgBQEpEyTQPKAFAQkIGlAkoAE\nJA1IEpCApAFJ9ggkIGlAAlIQkDJNAskDUhCQgKQBSQISkDQgSUACkgYk2SOQgKQBCUhBQMo0\nCSQPSEFAApIGJAlIQNKAJAEJSBqQZI9AApIGJCAFASnTJJA8IAUBCUgakCQgAUkDkgQkIGlA\nkj0CCUgakIAUBKRMk0DygBQEJCBpQJKABCQNSBKQgKQBSfYIJCBpQAJSEJAyTQLJA1IQkICk\nAUkCEpA0IElAApIGJNkjkICkAQlIQUDKNAkkD0hBQAKSBiQJSEDSgCQBCUgakGSPQAKSBiQg\nBQEp0ySQPCAFAQlIGpAkIAFJA5IEJCBpQJI9AglIGpCAFASkTJNA8oAUBCQgaUCSgAQkDUgS\nkICkAUn2CCQgaUACUhCQMk0CyQNSEJCApAFJAhKQtC0EUnGhX1l5YXjVNY7F8jLHYk21Y7Gi\n1LFoqxyLlSXha0U24ZhMFIevFdtKx2SVY7LEVjgmq4vC10qt68C7jnuZdR54x1q5dRz4Iucl\nZh0Hvth5iVnX4XNeYtZ1+JyXmHUsluklVpRLSKVSZWVpeDU1jkXnpK12LCYq4k5WlYevldmq\nmJPlNuGYrC6LO+k6ehXWeeAda5XWdfhiT5Y5LzHrOvDOS8y6Dp/zEnNNVjgvMetYrJ3MJSRu\n2tXGTTuPm3ZASgakTJNA8oAUBCQgaUCSgAQkDUgSkICkAUn2CCQgaUACUhCQMk0CyQNSEJCA\npAFJAhKQNCBJQAKStkVBKnvrMc8mgBQakICkuSH9uZMxc+w1Z0WiBKTagOQBSZti+v/dh/Sv\nbW4CUkhAApLmhHTgr22ZD8le/QMghQQkIGlOSO1eCCA93xpIIQEJSJoT0k5PBZD+sx2QQgIS\nkDQnpOOPKRVIa/fvB6SQgAQkzQnpv632vticfeZ2rV8FUkhAApLmvvt71sHGr/f/ojgCUjIg\neUBKtuq999bZaAGpNiB5QNJ6fRy8f+SHQAoJSEDSnJDMXH2XGNsGSCEBCUiaA5Kp78dACglI\nQNIckOb91Zx0jnTu75YAKSQgAUlz3rT7v8+C90WfASkkIAFJy+rvkWZ1AVJIQAKS5oY0Y1jf\nPn36HN6pK5BCAhKQNCekB8w2u5td25mfPg2kkIAEJM39e6QTCm2r+Ylbf1IIpJCABCTNCanT\nDGtbfWDtJaOAFBKQgKS5/x7pWWu3e8Xa2bsCKSQgAUlzQjr41Aq737XWPtkBSCEBCUiaE9K9\n5jg7ptXIsbsdCaSQgAQkzX339wMTbMnPjOk+F0ghAQlIWha/kP3848oojoCUDEgekJIVrteA\nFBKQgKQ5IS38ZYfah38DKSQgAUlzQvpJ52GXX6kBKSQgAUlzQurwWhRAQGoQkDwgaTstAxKQ\nMk4CycsA6bIbgQSkjJNA8jJAqji+z+UTNCCFBCQgaU5IE+qetAFIIQEJSJoT0i4DX/3iSw1I\nIQEJSJoTUlvubABS5kkgeZke/T0PSEDKOAkkLwOkl499H0hAyjQJJC8DpD67m449NCCFBCQg\naU5IfY9LBqSQgAQkLavntYsWkGoDkgckICUDUqZJIHkuSD3H2551ASkkIAFJC4d02CR7WF1A\nCglIQNK4aScBCUgar9gnAQlIWrODxCv2eUDKPAkkj1fsCwISkDResU8CEpC0Zgap/hX7ogWk\n2oDkASmoZLm1pff8eSGQwgISkDQnpE92mmAThxjT+V0ghQQkIGlOSKcc8IW919zxxZGnAikk\nIAFJcz8d133Wnry/tfd1B1JIQAKS5oTU5iVbtcMV1s7k90hhAQlImhNS97vtTPOStVN3AVJI\nQAKS5oR0zs5X9diryq46kJ+RwgISkDQnpOWHm65zrB3SOdJTNwCpNiB5QKqtQF5jbO7KKI6A\nlAxIHpCsXV9RtzlvEpBCAhKQtHBIZrL/pujKBf7byTxlcVhAApKWAdIK8yyQgAQkCUgekIKA\nBKQgIHlAkoAkAckDkgQkINUGJCBpQJKABCRto0C6bM6cOU+bSf7by4AUFpCApDkgpQakkIAE\nJC0c0vWpASkkIAFJi/pMq0UTzxw6dlWwvWL8GYP+uCH9PCDVByQPSGHdeOWiZTePqpbNyl/d\nuHTxdVennQeklIDkASkkr/9C/zvQAH1t2QV5a/wz8hanngeklIDkASmk1wfW+G8veEi2P8wr\ntLZqwKzU84CUEpA8IIX03Fny9top8rZ02N8TifsGPJ5y3sJb/b4o9auoLA2vpsaxWFnhWLTV\njsVE/Mny8LUyW+WYrHJMltuEY7K6LO6k6+hVWOeBd6xVWtfhiz/pPO7WdeBdk1XWdficl5hz\n0nmJWcdi7RW+aUiVjSGNqIdk5583YMh9501POe+lXn5vui0SbbHV3VGQDqnrRe80+MA3gptx\nD9eeLE4kTp6Tcl7hx36r1vuVlK0Pr7rasVhW4lisqXIsljsmN9iEY7KiKHytwFY6JisLw9cK\nbYVjMlEQvlZknYdvQ/hasS11TTrWSq3rwDsvMVscvrjBeYlZ14F3XmLWcfgKXce90roOn/MS\ns47F4Apf0DSkn7Q0+9+0PPWctf0/t7bgpA9lu+oV/xbhWwMKUs+T+BmpNn5G8vgZKWjF5L4t\nWp3wQP0tPzvhkkVLb7i0xs6cbu3F470Pz7yj/jwgpQckD0h1LbvlULPduXU/95RMyh823v/w\nm67zl6459Yy7EvXnASk9IHlASunDocaYI+c2XggJSLUByQNSspUTDzStfvno9ENaPQekJgIS\nkDQnpIpH8rYxPSfI3Q2VJ+wFpCYCEpA0J6QupuOI2bXbT7QAUhMBCUiaE9JRU4vrtr+eCqQm\nAhKQtAx3f9/qv1md8icSQGoQkICkOSF9urO8MNJis3OkF5EFUm1A8oCkDdj7LXn38d6nACkk\nIAFJc0L6zj+C93d2AlJIQAKS5oS07bTg/X3tgRQSkICkOSEd+X9V8q7w0D5ACglIQNKckJ5r\n8b1RN/xuxHdaZv2oBiClBCQPSEEze8mT2h34dBRHQEoGJA9IydZ88FGhLfoMSCEBCUhaVs/Z\nMKsLkEICEpA0N6QZw/r26dPn8E5dgRQSkICkOSE9YLbZ3ezazvw00g9JQKoNSB6QtF4nFNpW\n8xO3/qQQSCEBCUiaE1KnGda2+sDaS0YBKSQgAUlzQmr3rLXbvWLt7F2BFBKQgKQ5IR18aoXd\n71prn+wApJCABCTNCelec5wd02rk2N2OBFJIQAKS5r77+4EJtuRnxnTP/imEgFQfkDwgpfT5\nx42fAxxItQEJSJoT0hHRHmQHpPSA5AFJ230ikICUcRJIXgZIT/7w8Wi36oCUGpA8IGl9DzBt\ndu0hASkkIAFJc0Lqc+xxtQEpJCABSYv6quZASg9IQNKAJAEJSNpGgrRjMp6OKywgAUlzQjpJ\n673t/jz6OywgAUnL5qbdiqNnACkkIAFJy+pnpLm9gBQSkICkZQVpxbZACglIQNKygVQzbncg\nhQQkIGlOSAdp+3c1lwMpJCABScsC0sHH/rUCSCEBCUgav5CVgAQkjZe+lIAEJK3ZQeKlLz0g\nZZ4EksdLXwYBCUiaE9Liz1eFL/LSlxKQgKQ5ID29jzFdbw9d5qUvJSABSQuH9G5neakw82DY\nOi99KQEJSFoopNVnqiPTK2ySl76UgAQkLYC05PP5s2c8Mu22caMvGjm43zG9e3bbvmXgyGwf\nNslLX0pA2mohLZr/5qzHpt058frLz88f8LOjDt5jpw6mQW279Nj/sJ2D7b3DdpvVS19GC0i1\nAcnbWJCWzHx0bvhqGCT9TvPs9DvGjRk9cnD/fr179ui2TSM13Xoe1Lvf4PyRo8dNvG3ajFmz\n5y/T2SeC5d+FfU5+ISsBafOC9Nhu/nV60LKwZYW0ZP7sWTOm3TZxXB2a1o3R9OjZu1//wSNH\njxkXoFkVfond0EY+Z+gd4PxCVgLSZgVpfhd1MKrujBQ0+YP79f7RD7u1CUVz3R/H3zbtkRmz\n53++uvGeXb9HemfKbS+Er/ILWQlImw2kFZ+/88o5gY1tBp94TK99um/fqiGa7Xf//o+O+cWg\nMy+6+g9/mfrQUy+9/emS+h1sikc28AtZD0iZJzcepC8/DO4J+N1lvx7e/7jDD+jRpW1DNNvt\nstdBR59wSv6FV/5+4t0PPDnr7Y+/an4PEeIXsh6QMk/mBtLyz99++Zn/TP3r+GsvOmvQL445\n6Pu7bt8QTbsdexx4xPEn5fcJTraf8+GXTe622UHiF7LeVgNp2WN3PLEi1uTXj/ztqfAHoTUN\nqfY3NXfdclXdb2oa/1CzZ+09Ael3n/l9upN+xOiwz9nsIPELWW9rgTR7L/+auc+bMSaf7+5P\n/mh+2LIPqfH9Z03d6dz7mH6DR140etxtU6fJPQGrXPfaPbu3Ma3ODXXf7CDxC1lvK4G0bB+9\nPh+4MvLk4u46eXTtSf1WM21qyoMC2jV9/5n+pua2fz2Y9q0mLdfvkVa88cKn4avND1LyF7LF\nQAppC4FU++tGc/aYkCaELQyqnTy670Hf69axIZoOO+3546P6nZx/wZW/nzj1wekvpt1/1iwe\n2ZDexv1T8zdHcq9dWFsCpI/uv+KghgJitM323/3hoT/pf/q5l4y56fZ/PvLCa/M+95r/Q4TS\n24iQ1t5ygDF9gRTSZg7p0wev/vmuKRhueSSkmWELY4LBlq8ubfpTAkmqeeG0tmbXqz6L4ghI\nyZo3pIUzxg3u2cI3sF3vkbfNPk45nJjVZForDtXJ4WHrQLJ2yY17mrYnmhciMQJSXc0W0qKk\noU5iSB8ps6C/MS1O/SLTZBO9f7z//Sj/67BlID32i1bmwFvWeEDakiB9KYbkj2s61hmqbfGb\noRg89y9kv5zrmASS2eG3b/vvgLSlQFo8Y9wZaqiDGGr0K9RN/ciGRm0pkDqYg/+0DEhbBKRl\nsycO7imP7WzfpCENSN7GgVRwx8Gm1S8eWQ6kzRrS8tm3jewtf1TQuufgcW+F/sIVSNpGutdu\n7nmdzPbmga0a0rtnHfazP4df/5ozpDRDM+ThA835QauN2oIgWVt016HGHHF30VYLaXZ7993C\nzRTSCjEkf3KwTdKQBqRN+MiGeed3Nh22WkiHBb9vvCdsvflB2iCG2iUNpf+aFEib9CFCJfcc\nsbVCWlb7NEwH3zzthflN3cBrVpBWvHzryMPU0H7DbprZxENBgcTLugR965BapDx+ptXOP/q/\ns6685f7/fVz/Ac0F0srZk889dFv5Ivc9bcKzSxp/gAYkIAV925BWB38kYK6ZOu6i/r171D0P\nzfYH9csfPXHarPllzQDS/Gmj++nfxXXrP+754s31L2QbBSTZ45YBafGJRr8l/TJ5xuezH7lt\nzEifVPJP09p2S5JqNPz40d32uyrkoZxejiCJoR3UUL/R0z7zNuc/NW8UkGSPWwSkuT80hz6f\n3+u4PzXxt5i1pI7co1U9qf4jx9z2yOyFwQfcr+ee0MRzPgV9U0hiqEvS0ILkGpA8IAU1H0hP\n7mjyl2X8heyy+bOmTRyd34ihGZEAABwnSURBVO+gbsnnh27bo7dPaofgxH1hk98A0oe+oR0b\nGdKA5AEpqNlAuqFV65u8KI9sWPr2jLvHXTDoqB+k/qHofpf96d7nm7rDLx6k9/55yU/VUPcT\nr334s8brQPKAFNRMIC0ZaL7zlGzEeYjQV3Om/zntr0dbdZM7/P76wMuf1H1MZEgf3HvZ8V1l\nZ7v/4uqHwp6xAEgekIKaB6T3DzYHvKtbcR9r1ysgdO+MqeMuGnxM3R1+bZL3Tnzl+LVqQ0if\nzxjTr1vyttxHW+bz2jUKSLLHzRzSjO+YU2r/yCYupFf1Jd7qn3ut7g6/RvdOLGowufq2fbbp\n/tvaT/+F/AGRfHhneeC2ngUkIAU1e0gTW7cak9yO/ejvT0cPOPfxphZ8UhNH5//yoG7J3/YG\n905MnfH2cl3/k56Z1+CPwevngbR1QKqs8quurgrPWseie7LGsViTo8mSs82Oz+fgc1a7J6uq\niha8fP/ES4f27dm+VlSLnX/0y5HX1D6/tb60VZfjr/rPwgaTzqNnnQdhE0w6j8E3mHR9QZts\nMpFLSJv7d6T5h5h9364/+a38PdKi1x67fczIEw/tnvJyJO36Xnj3201N8h1p6/iOtJlDenZn\n0/+rlLVv+Q/7PnllUi2kP4dNAglIQc0Z0h3tWlyU9rfY3/pfyK7eVx1t+17YJJCAFNR8Ia24\nyHT8d/rat/+n5i/LI3/aTA6dBBKQgpotpAV9zfdea7C2CZ6z4bPfj7iy4ZeREpCAFNRcIf23\nuzm+0XMkbuZPfpIWkIAUtFEhTdm2wY9HGpAyTQLJA1KQQFp5kenQ1FMzACnTJJA8IAX5kD77\nqdnjlabWgJRpEkgekILKNsz5vjmy6UdUAynTJJA8IAWVPdjJ5Ie8JCmQMk0CyQOStvoPLduG\n/toGSJkmgeQBSfryl2bXmaGrQMo0CSQPSH5v9jRHNvFH28mAlGkSSB6QPO/Bzia/cMt9NYq0\ngASkoJxDWj2mZZtJW/LLuqQFJCAF5RrSV3lm52e36NdHSgtIQArKMaR5B5kD5a8VgAQkDUhS\nZEjTu5pB+nzzQAKSBiQpKqSJrbepfYoTIAFJA5IUDdKy00yXx2q3gQQkDUhSJEgf/Njs/27y\nBJCApAFJigLp6Z3MyV/XnQISkDQgSREgTWxT/wyQHpAkIHlACsoa0rLhZof/pK4CCUgakKRs\nIX3ax+w9J20VSEDSgCRlCenF3U2/Bs9bDyQgaUCSsoP0t4bPAOkBSQKSB6SgbCDJM0D+q9Eq\nkICkAUnKAtKCo833Xm28CiQgaUCSMkP633fNcY2eAdIDkgQkD0hBGSHd1b6pZ4D0gCQByQNS\nUAZIKy9q0f4fTa8CCUgakCQ3pDUnmN1mhawCCUgakCQnpE97miM+CVsFEpA0IEkuSA90NvnL\nQ1eBBCQNSFI4pNVjWra92zEJJCBpQJJCIS0+0ezyZtYvxtwgIGWaBJK3dUB6ax/T++PsX9W8\nQUDKNAkkb6uA9ND2Jn9Z9q9q3jAgZZoEkrc1QBrXqs1fIryqeaOAlGkSSN6WD2npYLPjkx6Q\ngoAEpKCokOb9KHgGSCBpQAJSUERI07uaU/UZIIGkAQlIQdEg1T8DJJA0IAEpKAqkZUNNl0eT\nJ4AkAQlIQREgze9l9nun7hSQJCABKSh7SM90MwPqnwESSBqQgBSUNSR5BsjVKWtAkoAEpKAs\nIa24yHS6N20NSBKQgBSUHaQFR5m9X09fA5IEJCAFZQXppe7mZwsbrAFJAhKQgrKBdOe2TTzF\nCZAkIAEpKDOklReZDv9svAYkCUhACsoIacExZs/ZTawBSQISkIIyQXr5u+bYz5taA5IEJCAF\nZYB0d/sWF61scg1IEpCAFOSEVDK2RdvbQ9aAJAEJSEEuSF+eaHZ7IWwRSBKQgBTkgPTGD0yf\n0GeABJIGJCAFhUN6YDtztuMIAUkCEpCCwiCtHtOyzV8jvKp5g4AEJG1rhyTPAPlc9q9q3igg\nAUnbyiHN/aE59KPsX9W8cUACkrZ1Q/qPPAOkByQJSB6QgiJDmrhN7VOcAAlIGpCkiJCWDjFd\nngg2gQQkDUhSNEjvH2wOeLd2G0hA0oAkRYI04ztmYN1TnAAJSBqQpCiQJrZuNab+FJCApAFJ\nyh7SsmGmyyMpp4EEJA1IUtaQ5h9i9n0ndRFIQNKAJGUL6Zlu5qSv0haBBCQNSFKWkO5o2+Ki\n1emLQAKSBiQpK0jyDJD/brgIJCBpQJKygbSgr9nrtUaLQAKSBiQpC0j/7W6Ob/gMkB6QJCB5\nQArKDGlKU88A6QFJApIHpCAXpA8en7FAngHyniZXgQQkDUiSA9LoNsZ07Gn2eKXpZSABSdsc\nIBVNPHPo2FXB9pLfDxty1UfWXpjnN2jjQ7rDaD9u8hkgPSBJQPI2D0g3Xrlo2c2jqmWzZuTk\nkvJpgwrtiKc8ub5tdEi9AkinhK0DCUjaZgDJ67/Q/640YJ6CyfvE2nV5C+ypc9M+ZqNB2jWA\ndETYOpCApG0GkF4fWOO/veAhPTF6UmHZ/edWVObdevHZ45fKOYkCv3Vr/IpL14RXXe1YLC0O\nWzksgDQkbL2sKHyva22l43OWF4SvrbMVjsmKDeFrPiTHZOW68LUC6zp8VWvD1wptiWPSddyL\nrePwOSdLbGH44tqEY7LMug6f8xKz68MXN5Q7Jn1I4YuFrsmEdSwW6SW2PltIz50lb6+doifW\njsrLy//Cbhj+lwULbhhe7J/zUi+/N927iN/j6qjdOxtr/0TfsOq6rUyQRsjbAFLi4skbSh4e\nFnw3Kx0003877zd+8yv9qqoqw6upcSw6Jn/hO+r2QOhytetzWtfnrE6EryVs9UaZdH098Ser\nrPPAN7PJaus4fPEnE86jZx2L8SeDq21FtpDeCG7aPSzb7/Yv89+ePT1YOf/+5MdstJ+RlnZr\nP/21r8NW+RlJ4mckb7P4GWlt/8+tLTjpQ9l+J6/Ef5s/ffHkhLVlg17a6JD+ZM7P8lXNGwck\nCUjNBZKdcMmipTdcWmNnTrcl+ZOLKh4duLxw6KQVS8ePKN/YkFbu2fo9IHlAkjZ7SCWT8oeN\n9z/8puusXTx22GlXfGDtwuuGnHHjyroP2ViQ/maGZfeq5k0FJAlIzQZSFm0sSAe0fA1IEpCA\nFBQL0v0mL5tXNQ8JSBKQgOQdZp4FkgYkIAXFgfSsOcYDkgYkIAXFgdTPPOoBSQMSkIJiQHqt\n5cHyDkgekCQgSTEgnWr0r2KB5AFJApIUHdI72+ytz9IAJA9IEpCk6JDOMX/V90DygCQBSYoM\n6ZN2uyzTDSB5QJKAJEWGdIm5MdgAkgckCUhSVEhfdt5hcbAFJA9IEpCkqJB+Z0bXbgHJA5IE\nJCkipGU7b/tp7SaQPCBJQJIiQppozktuAskDkgQkKRqkld9rnXzdciBJQAJSUDRId5nT6raB\n5AFJApIUDdJBLWbXbQPJA5IEJCkSpIfML+pPAMkDkgQkKRKkPuaZ+hNA8oAkAUmKAul5c1TK\nKSB5QJKAJEWB9HPzn5RTQPKAJAFJigDp9Zb7rU45CSQPSBKQpAiQTjN3p54EkgckCUhS9pDe\nb9NjZeoikDwgSUCSsoc00vwlbRFIHpAkIElZQ1rQYaelaYtA8oAkAUnKGtLl5vr0RSB5QJKA\nJGUL6asu2y1MXwSSByQJSFK2kH5vfttgEUgekCQgSVlCWr57248aLALJA5IEJClLSLeYcxsu\nAskDkgQkKTtIq3q2mttwEUgekCQgSdlBuscMarQIJA9IEpCk7CAd0uKVRotA8oAkAUnKCtIj\n5v8aLwLJA5IEJCkrSEebpxsvAskDkgQkKRtIM83hTSwCyQOSBCQpG0gnmgeaWASSByQJSFIW\nkOa03Hd1E4tA8oAkAUnKAtJQ8/emFoHkAUkCkpQZ0gdtvruiqUUgeUCSgCRlhvQbc1OTi0Dy\ngCQBScoI6bOOXZc0uQgkD0gSkKSMkK401zW9CCQPSBKQpEyQvt6x0xdNLwLJA5IEJCkTpHHm\n4pBFIHlAkoAkZYC0vHubD0MWgeQBSQKSlAHSZHNW2CKQPCBJQJLckAr3afVW2CKQPCBJQJLc\nkB4wp4QuAskDkgQkyQ3pMPNi6CKQPCBJQJKckJ4zx4UvAskDkgQkyQnpeDM9fBFIHpAkIEku\nSP9rcYhjEkgekCQgSS5IJ5mHHJNA8oAkAUlyQHq7VU/HpQIkCUhACnJAyjd3uy5PIHlAkoAk\nhUP6uO1uG4AEJAlIsseYkC4wE0qABCQJSLLHeJAWbtflKyABSQOS7DEepGvM1R6QgKQBSfYY\nC9LSndovAJIHJA1IssdYkP5oRnlA8oCkAUn2GAfSyj3afAAkCUgekOJDusMM94AkAckDUmxI\nq/dv+YYHJAlIHpBiQ7rP9Jd3QAKSBiTZYwxIvc0seQckIGlAkj1Gh/SU+am+BxKQNCDJHqND\n+pl5TN8DCUgakGSPkSG93OLgYANIQNKAJHuMDGmg+WewASQgaUCSPUaF9M42e68KtoAEJA1I\nsseokEaYW2u3gAQkDUiyx4iQPmm367LaTSABSQOS7DEipEvMH5KbQAKSBiTZYzRIizrvsDi5\nDSQgaUCSPUaDNMaMrtsGEpC0rR5SSZFfeUVReDU16afX7Nx+cd2JinLXZLVjsbLMsWirXJOl\n4WvFNuGYTJSEr5XYSsdkVXH4Wql1Hb5qx2SZdR4+x1p57MkK6zjwxc5LzDoOfInrEktYx4Ev\ndV5i1nX4nJeYdSyW6SVWnEtIpVJlZWl4NTXpp281F9afcE7aasdioiLuZFV5+FqZrYo5WW4T\njsnqsriTNY61Cus88I61Sus6fLEny5yXmHUdeOclZl2Hz3mJuSYrnJeYdSzWTuYSUuSbdiu/\n1/q9+lPctOOmnbbV37SLDGmKOT3lFJCApAFJ9hgF0oEtZqecAhKQNCDJHiNAetCcmHoSSEDS\ngCR7jADpSPNM6kkgAUkDkuwxe0jPmb5pi0ACkgYk2WP2kE4wD6ctAglIGpBkj1lDeq3l/qvT\nFoEEJA1IssesIQ0xU9MXgQQkDUiyx2whzWuzx8r0RSABSQOS7DFbSOeaSQ0WgQQkDUiyxywh\nLWi/09IGi0ACkgYk2WOWkC41NzRcBBKQNCDJHrOD9FWX7b9suAgkIGlAkj1mB+kGc1mjRSAB\nSQOS7DErSMt32/bTRotAApIGJNljVpAmmXMbLwIJSBqQZI/ZQFq1V+t3Gy8CCUgakGSP2UCa\naoY0sQgkIGlAkj1mA+lHLV5pYhFIQNKAJHvMAtLD5oSmFoEEJA1IsscsIPU1Tze1CCQgaUCS\nPWaGNNMc2eQikICkAUn2mBnSiebBJheBBCQNSLLHjJDmtNxvdZOLQAKSBiTZY0ZIp5spTS8C\nCUgakGSPmSC936bHiqYXgQQkDUiyx0yQfm1uDlkEEpA0IMkeM0D6rMN3loQsAglIGpBkjxkg\njTZjwhaBBCQNSLJHJ6SK9V06fRG2CCQgaUCSPTogvXRUm1ZmaOgykICkAUn2GA7pne2MX8e3\nw9aBBCQNSLLHcEhDjDYobB1IQNKAJHsMh3RAAOmHYetAApIGJNljOKQjAki9w9aBBCQNSLLH\ncEgTAkjjw9aBBCQNSLLHcEirThRHJ64KWwcSkDQgyR5dv0d68Iormv4LCg1IQNKAJHuM8mLM\n6QEJSBqQZI9AApIGJCAFASnTJJA8IAUBCUgakCQgAUkDkgQkIGlAkj0CCUgakIAUBKRMk0Dy\ngBQEJCBpQJKABCQNSBKQgKQBSfYIJCBpQAJSEJAyTQLJA1IQkICkAUkCEpA0IElAApIGJNkj\nkICkAQlIQUDKNAkkD0hBQAKSBiQJSEDSgCQBCUgakGSPQAKSBiQgBQEp0ySQPCAFAQlIGpAk\nIAFJA5IEJCBpQJI9AglIGpCAFASkTJNA8oAUBCQgaUCSgAQkDUgSkICkAUn2CCQgaUACUhCQ\nMk0CyQNSEJCApAFJAhKQNCBJQAKSBiTZI5CApAEJSEFAyjQJJA9IQUACkgYkCUhA0jYHSEUT\nzxw6dlWwveT3w4Zc9VH6eUCqD0gekMK68cpFy24eVS2bNSMnl5RPG1SYeh6QUgKSB6SQvP4L\n/e9AA+YpmLxPrF2XtyD1PCClBCQPSCG9PrDGf3vBQ3pi9KTCsvvPrUg7D0j1AckDUkjPnSVv\nr52iJ9aOysvL/yL1vAXj/D4r86tMlIVXU+NYTFQ6Fm38yWrHYlVF+Fq5rXJMVsefLA9fq7DO\nw+dYq/wGk67D5zzu1nEQylzH3TlZ7rzErOvwOY/7N5h0LNZe4bOGNKIeUuLiyRtKHh62LuW8\nl3r5veneBdEWW90dBZkgvRHcjHtYtt/tL/7Onp5yXuHHfqvW+5WUrQ+vutqxWFbiWKypciyW\nOyY32IRjsqIofK3AVjomKwvD1wpthWMyURC+VmSdh29D+FqxLXVNOtZKrevAOy8xWxy+uMF5\niVnXgXdeYtZx+Apdx73Sug6f8xKzjsXgCl+QLaS1/T+3tuCkD2X7nbwS/23+9NTzJH5Gqo2f\nkTx+RgprwiWLlt5waY2dOd2W5E8uqnh04PK684CUHpA8IIVVMil/2Hj/w2+6ztrFY4eddsUH\n9ecBKT0geUD6BgGpNiB5QAJSMiBlmgSSB6QgIAFJA5IEJCBpQJKABCQNSLJHIAFJAxKQgoCU\naRJIHpCCgAQkDUgSkICkAUkCEpA0IMkegQQkDUhACgJSpkkgeUAKAhKQNCBJQAKSBiQJSEDS\ngCR7BBKQNCABKQhImSaB5AEpCEhA0oAkAQlIGpAkIAFJA5LsEUhA0oAEpCAgZZoEkgekICAB\nSQOSBCQgaUCSgAQkDUiyRyABSQMSkIKAlGkSSB6QgoAEJA1IEpCApG0hkDI3+a9xJ2+aEnMw\nMW5azMmCcY/GnFw17pmYk4vH/Tfm5Mfj3og5+c6492NOvjrus5iTL4xbEnNy+ri1MScfGlcS\nc/Kf42oyf1CybwXSL0+IO3nUkJiD5b1+FXNyVa8rYk4u6DUu5uRbvW6POflCr3tjTj7Sa3rM\nyam9Xo45+ZdecfFe32txzMlLeq2POTmiF5CAlDEgZQpIFkiZA1KmgGSBlDkgZar5QSLa0gMS\nUQ4CElEOAhJRDvoWIK29+YzBVy2IM/n1jUNPv+aTmJ92Vt6cOGMX5vkNivUZnz735AveijH3\nQZ42I8bokt8PG3LVRzEGV4w/Y9AfN0QeW3rZSfKuaOKZQ8euijNZ9z7yZIyrUe1kjKtR/VeZ\n7dXoW4D02ysXLv/zsLLog4kz/7J0+aTTSmN91vXDB8aCNOIpTx4mFKNZ+XNXPTEyxq/R9aEx\nHw36OvpkzcjJJeXTBhVG/5S/unHp4uuujjr2Sv4kvYLdeOWiZTePqo4xmXwffTL61ah2MsbV\nqP6rzPpqtPEhFY73ryGr82I8qmTDY/4/fmnewlifdsLU4bEgnTo31qfzG/li3EnpuvtjDG3I\n8/+jXZcX/dv9grw11np5Ue9SfnH1HLmCef39y6RowLzok3XvI0/GuBrVTsa4GtV/lVlfjb6l\nn5E+OWld5g9qqsI7flMZZ+71c8tiQarMu/Xis8cvjTG5Ju/FC0+9LO4N0VfOScQZGz2psOz+\ncysiz32Y538XqxowK/KgXsFeHyi/YLngoeiTKe+jT0a/GtVNRr4a1U5mfzX6diAVnn9PrLnq\nU/KuXhNnsCj/PRsL0obhf1mw4IbhxdEnF+RdvaRwymnRf+6Qqn/9Qqy5taPy8vK/iD5XOuzv\nicR9Ax6PPKhXsOfOks1roz2gOAeQIl+NaidjXI2CyQhXo28F0pLz7ojwO+K0yQ8mnFcUY+6W\nW2w8SFrpoJnRhxbk+Td1qk6P/p+89MpZVXHGEhdP3lDy8LAY3+3nnzdgyH3nRX9sQwBphGx+\n65CiX43qJiNfjYLJCFejbwPSvKFPxR+uHhLj3qz38gu/CSR7fowfWLy8z/23ox6O9QnHxvtz\nkXf7yw/fZ8d6rE9xInFy9COkV7A3gpt20f6p3xhSjKtR/eeKejXSyShXo28B0kenvx1v8N2R\n5dbWDIsB6aaBQ4cO7T94fPTJxZP9n1XKBr0UfbI637+cKwa/En3Sv1JH+sG9vnfy5E7C/OiQ\nql7xv4u9NaAg8qBewdb29//PKDjpw+iTNj6kOFcjnYx1NdLJKFejjQ+pYuQDcvdujLu/i4b/\n8esVUwauiD6pfzp7xszo1xNbOHTSiqXjR5RHn7QPD3vPuzU/xr/T/882L9ovZZKV5E8uqnh0\n4PLokxeP9z48846oU+u8mSfJRTnhkkVLb7g0yu2s5GTyfeTJGFej2skYV6PayShXo40PaV78\nXzcuvn7Q4Mvj/V9t4960W3jdkDNuXBlnsvpfw0++KsYvg/z+2z/WfXb+ERo77LQrPogxuOya\nU8+4K/InPUcvyidtyaT8YeMj/WSWnEy+jzwZ42qU/FzRr0apX2XzuWlHtOUHJKIcBCSiHAQk\nohwEJKIcBCSiHAQkohwEJKIcBKTm2vXm8OChA72OSz37sJ6pp47rkdzqk3Y+fdsBqbl2vTF3\n6kY6pElpj/wCUnMJSM2169v9fIfVspEOKT0gNZeA1Fy73nzW7kzZUEj/O77TtgdPtcFNu+rr\nd2/745kXtPYh7bXohI4dB6/1Ie3zzlHtd8iXZxV9pm/HdvtN9G8X9un71O5H2OXnfrdtt1Pi\n/ukuZReQmmvXm/KxRp4bWCDNanX0UzN/bf4cQBpnBj9/9y69O/iQ9jho/BOXtzjLR7N7z5se\nH90iz9rHW5zwxKxLzWhrjz1wn9tn2MN3vvul+w7YKe6Lm1BWAam5dr0pK//+vpUBpIP3Fgf9\nO5UJpJpu+/vfbt4wAsk85p9/5E4+JPOIvzXUfGX3+a48h8OA1muC1QJzlX/yi/HLNum/ZosP\nSM01H5J93vxRIa0yF5f5/d28JZCWm9/KB+wvkNrJPXv5LX1IbeXZPe4xjy0zv5bVqWaGPa6N\nf17ljj1mRXniLIoVkJprAskOar9YIL1nantMIL1nbpIPGNgheWfDOf6l2GdP2XrW3PmWuVG2\nnjFT7HG7ytare5odB94X8w+eKMuA1FxTSEs79beHCKSz52ieQHpDflay9tR0SHvJ1jPmrrlm\nrGw9be5O3qdX9eLl+5pD4j3PJmUZkJprCslONE8ecZxda85Mnu1D+kzuSLD2gHRIHeT22z/M\n9BXmPDlvinku5c5xe4f557f3pW+NAam5FkBKHNDjyOOs7d1Z7tf+17UJgZTovL9/4i2TDsnI\nU4gNaLnC7r+rDJ7QviBYfXuIPB/EF+bmTfTv2EoCUnMtgGRfbdHCh/S/1gf+6/nrWp8V3P19\nqTnr+Tv36JMG6Yjdf/C3WVea0/0bdS37Pfnsb8yE2tUVnQ6c+sKDR24X42kkKfuA1FyrhWRH\nGPmF7OyfdWr9g5sSAaTyC7t26Pvm0I6pkH58xNtHbbvDufI0iDOP6tD24H/Y5Or7J+/UeteT\n3900/4qtJiBtth23y6b+Cqg+IG2GTTrF/9a0vvPPN/XXQfUBaTPs3+bEJx86okW8pxmnjRKQ\nNsf+fXCH9kc+vam/CkoJSEQ5CEhEOQhIRDkISEQ5CEhEOQhIRDkISEQ5CEhEOej/AakLAfOE\n4S0JAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the seed. Don't remove this!\n",
    "set.seed(1234)\n",
    "\n",
    "audit_recipe <- recipe(Risk ~ .,  data = audit_training) %>%\n",
    "    step_scale(all_predictors()) %>%\n",
    "    step_center(all_predictors())\n",
    "audit_recipe\n",
    "\n",
    "audit_vfold <- vfold_cv(audit_training, v = 5, strata = Risk)\n",
    "\n",
    "knn_tune <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) %>%\n",
    "      set_engine(\"kknn\") %>%\n",
    "      set_mode(\"classification\")\n",
    "knn_tune\n",
    "\n",
    "knn_results <- workflow() %>%\n",
    "      add_recipe(audit_recipe) %>%\n",
    "      add_model(knn_tune) %>%\n",
    "      tune_grid(resamples = audit_vfold, grid = 10) %>%\n",
    "      collect_metrics()\n",
    "\n",
    "accuracies <- knn_results %>% \n",
    "      filter(.metric == \"accuracy\")\n",
    "\n",
    "accuracy_versus_k <- ggplot(accuracies, aes(x = neighbors, y = mean))+\n",
    "      geom_point() +\n",
    "      geom_line() +\n",
    "      ggtitle(\"Accuracy vs. K\") +\n",
    "      labs(x = \"Neighbors\", y = \"Accuracy Estimate\") +\n",
    "      scale_x_continuous(breaks = seq(0, 14, by = 1)) +  # adjusting the x-axis\n",
    "      scale_y_continuous(limits = c(0.8, 1.0)) # adjusting the y-axis\n",
    "accuracy_versus_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the increase of accuracy slows down when $K$ is greater than 6, so we decide to choose 6 as our $K$. Now that we have explored our data, separated the data into training and testing sets, and applied cross-validation to choose the best $k$, we can build our final model.\n",
    "\n",
    "First, we build our model specification with the best value for $K$. Assign your answer to an object called `audit_spec`. Then, pass the model specification and the training data set to the `fit()` function. Assign your answer to an object called `audit_fit`.\n",
    "\n",
    "At last, we use our final model to predict on the test dataset and assign this to an object called `audit_predictions`, report the accuracy of this prediction, and store this in an object named `audit_metrics`. We also report the confusion matrix and and store this in an object named `audit_conf_mat`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "K-Nearest Neighbor Model Specification (classification)\n",
       "\n",
       "Main Arguments:\n",
       "  neighbors = 6\n",
       "  weight_func = rectangular\n",
       "\n",
       "Computational engine: kknn \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "══ Workflow [trained] ══════════════════════════════════════════════════════════\n",
       "\u001b[3mPreprocessor:\u001b[23m Recipe\n",
       "\u001b[3mModel:\u001b[23m nearest_neighbor()\n",
       "\n",
       "── Preprocessor ────────────────────────────────────────────────────────────────\n",
       "2 Recipe Steps\n",
       "\n",
       "● step_scale()\n",
       "● step_center()\n",
       "\n",
       "── Model ───────────────────────────────────────────────────────────────────────\n",
       "\n",
       "Call:\n",
       "kknn::train.kknn(formula = ..y ~ ., data = data, ks = ~6, kernel = ~\"rectangular\")\n",
       "\n",
       "Type of response variable: nominal\n",
       "Minimal misclassification: 0.1295337\n",
       "Best kernel: rectangular\n",
       "Best k: 6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 193 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>.pred_class</th><th scope=col>Money_Value</th><th scope=col>TOTAL</th><th scope=col>Risk</th></tr>\n",
       "\t<tr><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>  0.94</td><td> 4.83</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>  0.00</td><td> 0.74</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td> 41.28</td><td>40.17</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td> 63.18</td><td>51.64</td><td>1</td></tr>\n",
       "\t<tr><td>0</td><td>  6.78</td><td> 0.75</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>  1.16</td><td>19.03</td><td>1</td></tr>\n",
       "\t<tr><td>0</td><td>  0.00</td><td> 0.43</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td>126.13</td><td>15.38</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>156.92</td><td> 9.51</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>  2.29</td><td>18.05</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>  2.51</td><td> 1.61</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td>  0.16</td><td> 1.05</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td>  8.91</td><td>24.87</td><td>1</td></tr>\n",
       "\t<tr><td>0</td><td>  0.00</td><td> 0.89</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td> 53.34</td><td> 3.98</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td> 11.16</td><td>84.26</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td> 10.69</td><td>12.30</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>  5.69</td><td>84.73</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>  0.74</td><td> 7.37</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>  0.00</td><td> 3.72</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>873.37</td><td>97.11</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>  0.00</td><td> 1.74</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>  0.00</td><td> 0.58</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td> 36.52</td><td> 1.30</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td> 17.16</td><td>13.99</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>  0.00</td><td> 1.28</td><td>1</td></tr>\n",
       "\t<tr><td>0</td><td>  0.00</td><td> 0.85</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td> 10.79</td><td>73.66</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>  3.46</td><td>47.56</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>  0.00</td><td> 1.10</td><td>1</td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td>1</td><td> 0.00</td><td>1.8700</td><td>1</td></tr>\n",
       "\t<tr><td>0</td><td> 0.00</td><td>0.4900</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td> 0.00</td><td>1.6300</td><td>1</td></tr>\n",
       "\t<tr><td>0</td><td> 0.00</td><td>0.0000</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td> 0.00</td><td>0.0900</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td> 0.02</td><td>0.4900</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td> 0.00</td><td>0.4200</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td> 0.00</td><td>1.7400</td><td>1</td></tr>\n",
       "\t<tr><td>0</td><td> 0.00</td><td>0.2100</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td> 0.00</td><td>0.7800</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td> 0.00</td><td>0.2717</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td> 0.71</td><td>0.7400</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td> 0.58</td><td>1.0900</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td> 0.00</td><td>0.0000</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td> 0.00</td><td>1.0800</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td> 0.00</td><td>5.2400</td><td>1</td></tr>\n",
       "\t<tr><td>0</td><td> 0.00</td><td>0.4900</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td> 0.04</td><td>2.0000</td><td>1</td></tr>\n",
       "\t<tr><td>0</td><td> 0.00</td><td>0.6000</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td> 0.06</td><td>2.9900</td><td>1</td></tr>\n",
       "\t<tr><td>0</td><td> 0.00</td><td>0.0000</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td> 0.00</td><td>0.0000</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td> 2.40</td><td>0.7400</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td> 0.00</td><td>0.4025</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td>16.09</td><td>2.6600</td><td>1</td></tr>\n",
       "\t<tr><td>0</td><td> 0.00</td><td>0.8400</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td> 0.00</td><td>1.0700</td><td>1</td></tr>\n",
       "\t<tr><td>0</td><td> 0.00</td><td>0.0000</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td> 0.21</td><td>0.9000</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td> 0.00</td><td>0.8400</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 193 × 4\n",
       "\\begin{tabular}{llll}\n",
       " .pred\\_class & Money\\_Value & TOTAL & Risk\\\\\n",
       " <fct> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t 1 &   0.94 &  4.83 & 0\\\\\n",
       "\t 0 &   0.00 &  0.74 & 0\\\\\n",
       "\t 1 &  41.28 & 40.17 & 1\\\\\n",
       "\t 1 &  63.18 & 51.64 & 1\\\\\n",
       "\t 0 &   6.78 &  0.75 & 1\\\\\n",
       "\t 1 &   1.16 & 19.03 & 1\\\\\n",
       "\t 0 &   0.00 &  0.43 & 0\\\\\n",
       "\t 1 & 126.13 & 15.38 & 1\\\\\n",
       "\t 1 & 156.92 &  9.51 & 1\\\\\n",
       "\t 1 &   2.29 & 18.05 & 1\\\\\n",
       "\t 1 &   2.51 &  1.61 & 0\\\\\n",
       "\t 1 &   0.16 &  1.05 & 0\\\\\n",
       "\t 1 &   8.91 & 24.87 & 1\\\\\n",
       "\t 0 &   0.00 &  0.89 & 0\\\\\n",
       "\t 1 &  53.34 &  3.98 & 1\\\\\n",
       "\t 1 &  11.16 & 84.26 & 1\\\\\n",
       "\t 1 &  10.69 & 12.30 & 1\\\\\n",
       "\t 1 &   5.69 & 84.73 & 1\\\\\n",
       "\t 1 &   0.74 &  7.37 & 1\\\\\n",
       "\t 1 &   0.00 &  3.72 & 1\\\\\n",
       "\t 1 & 873.37 & 97.11 & 1\\\\\n",
       "\t 1 &   0.00 &  1.74 & 0\\\\\n",
       "\t 0 &   0.00 &  0.58 & 0\\\\\n",
       "\t 1 &  36.52 &  1.30 & 1\\\\\n",
       "\t 1 &  17.16 & 13.99 & 1\\\\\n",
       "\t 1 &   0.00 &  1.28 & 1\\\\\n",
       "\t 0 &   0.00 &  0.85 & 1\\\\\n",
       "\t 1 &  10.79 & 73.66 & 1\\\\\n",
       "\t 1 &   3.46 & 47.56 & 1\\\\\n",
       "\t 1 &   0.00 &  1.10 & 1\\\\\n",
       "\t ⋮ & ⋮ & ⋮ & ⋮\\\\\n",
       "\t 1 &  0.00 & 1.8700 & 1\\\\\n",
       "\t 0 &  0.00 & 0.4900 & 0\\\\\n",
       "\t 1 &  0.00 & 1.6300 & 1\\\\\n",
       "\t 0 &  0.00 & 0.0000 & 0\\\\\n",
       "\t 0 &  0.00 & 0.0900 & 0\\\\\n",
       "\t 0 &  0.02 & 0.4900 & 0\\\\\n",
       "\t 0 &  0.00 & 0.4200 & 0\\\\\n",
       "\t 1 &  0.00 & 1.7400 & 1\\\\\n",
       "\t 0 &  0.00 & 0.2100 & 0\\\\\n",
       "\t 0 &  0.00 & 0.7800 & 0\\\\\n",
       "\t 0 &  0.00 & 0.2717 & 0\\\\\n",
       "\t 0 &  0.71 & 0.7400 & 0\\\\\n",
       "\t 1 &  0.58 & 1.0900 & 0\\\\\n",
       "\t 0 &  0.00 & 0.0000 & 0\\\\\n",
       "\t 1 &  0.00 & 1.0800 & 1\\\\\n",
       "\t 1 &  0.00 & 5.2400 & 1\\\\\n",
       "\t 0 &  0.00 & 0.4900 & 0\\\\\n",
       "\t 1 &  0.04 & 2.0000 & 1\\\\\n",
       "\t 0 &  0.00 & 0.6000 & 0\\\\\n",
       "\t 1 &  0.06 & 2.9900 & 1\\\\\n",
       "\t 0 &  0.00 & 0.0000 & 0\\\\\n",
       "\t 0 &  0.00 & 0.0000 & 0\\\\\n",
       "\t 0 &  2.40 & 0.7400 & 0\\\\\n",
       "\t 0 &  0.00 & 0.4025 & 0\\\\\n",
       "\t 1 & 16.09 & 2.6600 & 1\\\\\n",
       "\t 0 &  0.00 & 0.8400 & 0\\\\\n",
       "\t 1 &  0.00 & 1.0700 & 1\\\\\n",
       "\t 0 &  0.00 & 0.0000 & 0\\\\\n",
       "\t 0 &  0.21 & 0.9000 & 0\\\\\n",
       "\t 0 &  0.00 & 0.8400 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 193 × 4\n",
       "\n",
       "| .pred_class &lt;fct&gt; | Money_Value &lt;dbl&gt; | TOTAL &lt;dbl&gt; | Risk &lt;fct&gt; |\n",
       "|---|---|---|---|\n",
       "| 1 |   0.94 |  4.83 | 0 |\n",
       "| 0 |   0.00 |  0.74 | 0 |\n",
       "| 1 |  41.28 | 40.17 | 1 |\n",
       "| 1 |  63.18 | 51.64 | 1 |\n",
       "| 0 |   6.78 |  0.75 | 1 |\n",
       "| 1 |   1.16 | 19.03 | 1 |\n",
       "| 0 |   0.00 |  0.43 | 0 |\n",
       "| 1 | 126.13 | 15.38 | 1 |\n",
       "| 1 | 156.92 |  9.51 | 1 |\n",
       "| 1 |   2.29 | 18.05 | 1 |\n",
       "| 1 |   2.51 |  1.61 | 0 |\n",
       "| 1 |   0.16 |  1.05 | 0 |\n",
       "| 1 |   8.91 | 24.87 | 1 |\n",
       "| 0 |   0.00 |  0.89 | 0 |\n",
       "| 1 |  53.34 |  3.98 | 1 |\n",
       "| 1 |  11.16 | 84.26 | 1 |\n",
       "| 1 |  10.69 | 12.30 | 1 |\n",
       "| 1 |   5.69 | 84.73 | 1 |\n",
       "| 1 |   0.74 |  7.37 | 1 |\n",
       "| 1 |   0.00 |  3.72 | 1 |\n",
       "| 1 | 873.37 | 97.11 | 1 |\n",
       "| 1 |   0.00 |  1.74 | 0 |\n",
       "| 0 |   0.00 |  0.58 | 0 |\n",
       "| 1 |  36.52 |  1.30 | 1 |\n",
       "| 1 |  17.16 | 13.99 | 1 |\n",
       "| 1 |   0.00 |  1.28 | 1 |\n",
       "| 0 |   0.00 |  0.85 | 1 |\n",
       "| 1 |  10.79 | 73.66 | 1 |\n",
       "| 1 |   3.46 | 47.56 | 1 |\n",
       "| 1 |   0.00 |  1.10 | 1 |\n",
       "| ⋮ | ⋮ | ⋮ | ⋮ |\n",
       "| 1 |  0.00 | 1.8700 | 1 |\n",
       "| 0 |  0.00 | 0.4900 | 0 |\n",
       "| 1 |  0.00 | 1.6300 | 1 |\n",
       "| 0 |  0.00 | 0.0000 | 0 |\n",
       "| 0 |  0.00 | 0.0900 | 0 |\n",
       "| 0 |  0.02 | 0.4900 | 0 |\n",
       "| 0 |  0.00 | 0.4200 | 0 |\n",
       "| 1 |  0.00 | 1.7400 | 1 |\n",
       "| 0 |  0.00 | 0.2100 | 0 |\n",
       "| 0 |  0.00 | 0.7800 | 0 |\n",
       "| 0 |  0.00 | 0.2717 | 0 |\n",
       "| 0 |  0.71 | 0.7400 | 0 |\n",
       "| 1 |  0.58 | 1.0900 | 0 |\n",
       "| 0 |  0.00 | 0.0000 | 0 |\n",
       "| 1 |  0.00 | 1.0800 | 1 |\n",
       "| 1 |  0.00 | 5.2400 | 1 |\n",
       "| 0 |  0.00 | 0.4900 | 0 |\n",
       "| 1 |  0.04 | 2.0000 | 1 |\n",
       "| 0 |  0.00 | 0.6000 | 0 |\n",
       "| 1 |  0.06 | 2.9900 | 1 |\n",
       "| 0 |  0.00 | 0.0000 | 0 |\n",
       "| 0 |  0.00 | 0.0000 | 0 |\n",
       "| 0 |  2.40 | 0.7400 | 0 |\n",
       "| 0 |  0.00 | 0.4025 | 0 |\n",
       "| 1 | 16.09 | 2.6600 | 1 |\n",
       "| 0 |  0.00 | 0.8400 | 0 |\n",
       "| 1 |  0.00 | 1.0700 | 1 |\n",
       "| 0 |  0.00 | 0.0000 | 0 |\n",
       "| 0 |  0.21 | 0.9000 | 0 |\n",
       "| 0 |  0.00 | 0.8400 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "    .pred_class Money_Value TOTAL  Risk\n",
       "1   1             0.94       4.83  0   \n",
       "2   0             0.00       0.74  0   \n",
       "3   1            41.28      40.17  1   \n",
       "4   1            63.18      51.64  1   \n",
       "5   0             6.78       0.75  1   \n",
       "6   1             1.16      19.03  1   \n",
       "7   0             0.00       0.43  0   \n",
       "8   1           126.13      15.38  1   \n",
       "9   1           156.92       9.51  1   \n",
       "10  1             2.29      18.05  1   \n",
       "11  1             2.51       1.61  0   \n",
       "12  1             0.16       1.05  0   \n",
       "13  1             8.91      24.87  1   \n",
       "14  0             0.00       0.89  0   \n",
       "15  1            53.34       3.98  1   \n",
       "16  1            11.16      84.26  1   \n",
       "17  1            10.69      12.30  1   \n",
       "18  1             5.69      84.73  1   \n",
       "19  1             0.74       7.37  1   \n",
       "20  1             0.00       3.72  1   \n",
       "21  1           873.37      97.11  1   \n",
       "22  1             0.00       1.74  0   \n",
       "23  0             0.00       0.58  0   \n",
       "24  1            36.52       1.30  1   \n",
       "25  1            17.16      13.99  1   \n",
       "26  1             0.00       1.28  1   \n",
       "27  0             0.00       0.85  1   \n",
       "28  1            10.79      73.66  1   \n",
       "29  1             3.46      47.56  1   \n",
       "30  1             0.00       1.10  1   \n",
       "⋮   ⋮           ⋮           ⋮      ⋮   \n",
       "164 1            0.00       1.8700 1   \n",
       "165 0            0.00       0.4900 0   \n",
       "166 1            0.00       1.6300 1   \n",
       "167 0            0.00       0.0000 0   \n",
       "168 0            0.00       0.0900 0   \n",
       "169 0            0.02       0.4900 0   \n",
       "170 0            0.00       0.4200 0   \n",
       "171 1            0.00       1.7400 1   \n",
       "172 0            0.00       0.2100 0   \n",
       "173 0            0.00       0.7800 0   \n",
       "174 0            0.00       0.2717 0   \n",
       "175 0            0.71       0.7400 0   \n",
       "176 1            0.58       1.0900 0   \n",
       "177 0            0.00       0.0000 0   \n",
       "178 1            0.00       1.0800 1   \n",
       "179 1            0.00       5.2400 1   \n",
       "180 0            0.00       0.4900 0   \n",
       "181 1            0.04       2.0000 1   \n",
       "182 0            0.00       0.6000 0   \n",
       "183 1            0.06       2.9900 1   \n",
       "184 0            0.00       0.0000 0   \n",
       "185 0            0.00       0.0000 0   \n",
       "186 0            2.40       0.7400 0   \n",
       "187 0            0.00       0.4025 0   \n",
       "188 1           16.09       2.6600 1   \n",
       "189 0            0.00       0.8400 0   \n",
       "190 1            0.00       1.0700 1   \n",
       "191 0            0.00       0.0000 0   \n",
       "192 0            0.21       0.9000 0   \n",
       "193 0            0.00       0.8400 0   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 1 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>.metric</th><th scope=col>.estimator</th><th scope=col>.estimate</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>accuracy</td><td>binary</td><td>0.8860104</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 3\n",
       "\\begin{tabular}{lll}\n",
       " .metric & .estimator & .estimate\\\\\n",
       " <chr> & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t accuracy & binary & 0.8860104\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 3\n",
       "\n",
       "| .metric &lt;chr&gt; | .estimator &lt;chr&gt; | .estimate &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| accuracy | binary | 0.8860104 |\n",
       "\n"
      ],
      "text/plain": [
       "  .metric  .estimator .estimate\n",
       "1 accuracy binary     0.8860104"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "          Truth\n",
       "Prediction   0   1\n",
       "         0  65  15\n",
       "         1   7 106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the seed. Don't remove this!\n",
    "set.seed(9999) \n",
    "\n",
    "audit_spec <- nearest_neighbor(weight_func = 'rectangular', neighbors = 6) %>%\n",
    "       set_engine(\"kknn\") %>%\n",
    "       set_mode(\"classification\")\n",
    "audit_spec\n",
    "\n",
    "audit_fit <- workflow() %>%\n",
    "       add_recipe(audit_recipe) %>%\n",
    "       add_model(audit_spec) %>%\n",
    "       fit(data = audit_testing)\n",
    "audit_fit\n",
    "\n",
    "audit_predictions <- predict(audit_fit, audit_testing) %>%\n",
    "       bind_cols(audit_testing)\n",
    "audit_predictions\n",
    "\n",
    "audit_metrics <- audit_predictions %>%\n",
    "        metrics(truth = Risk, estimate = .pred_class) %>% \n",
    "        filter(.metric == \"accuracy\")\n",
    "audit_metrics\n",
    "\n",
    "audit_conf_mat <- audit_predictions %>% \n",
    "      conf_mat(truth = Risk, estimate = .pred_class)\n",
    "audit_conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can see that our model has an accuracy of 88.6%, we will look at the confusion matrix for the classifier. This will show us the table of predicted labels and correct labels. A confusion matrix is essentially a classification matrix. The columns of the confusion matrix represent the actual class and the rows represent the predicted class (or vice versa). As we can see from the one we got, 7 Indian non-fraudulent companyies are predicted fraudulent and 15 Indian fraudulent companies are predicted other wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dicussion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the classification model using K Nearest Neighbours the optimal number of neighbors was found to be six. Optimal due to the high accuracy of the model when the K values were graphed against the accuracy estimate and low amount of overfitting. Knowing this, the estimated accuracy could be calculated from the confusion matrix, this accuracy being 88.6%. This accuracy indicates a strong correlation between the amount of money involved in misstatements in past audits (MONEY) and the total amount of discrepancy found in past audit reports (TOTAL). Meaning if you were to calculate an Indian firm based on their misstatements and discrepancies a new firm would have an accurate prediction of whether the firm was fraudulent or normal. Out of the 193 firms studied in the final model, only seven non-fraudulent firms were predicted to be fraudulent. In future models, the aim would be to bring this number down and find a more accurate model. The predictor model has small-scale applications, however, this makes it important when analyzing Indian firms who, in the past, have submitted audits.\n",
    "The difficulty would be how accurate this model is in the face of new firms that do not have a long history of being audited. If there is a collection of firms with almost no auditing data then the accuracy would nosedive. Therefore new predictors would have to be added in place of the old ones as too many predictors would make the data hard to visualize. While there are many multivariable methods of looking for fraud such as Beneish’s M-score (8 variables) or Zipf’s Law (Huang et al. 2008 , the best two predictor variables would be the firm’s growing revenue and the firms’ reported revenue. If there are discrepancies there is a clear signal of embezzlement. While this method is straightforward, collecting data on new firms is costly and there is not a breadth of historical data on these firms’ audit history. Meaning if you were to include new firms in your data would have to model the data to account for fairly new reported versus actual revenue values which, with the inclusion of your new data, would lower your predictor accuracy. These findings are quite useful however as they have provided an accurate prediction of fraud among a plethora of Indian firms. Future studies could use this model on firms from other nations with historically high amounts of fraud. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audit Data Data Set. UCI Machine Learning Repository: Audit Data Data Set. (n.d.). Retrieved December 8, 2021, from https://archive.ics.uci.edu/ml/datasets/Audit+Data. \n",
    "\n",
    "Bloomenthal, A. (2021, December 7). Detecting financial statement fraud. Investopedia. Retrieved December 9, 2021, from https://www.investopedia.com/articles/financial-theory/11/detecting-financial-fraud.asp. \n",
    "\n",
    "Hooda, N., Bawa, S., & Rana, P. S. (2018). Fraudulent firm classification: A case study of an external audit. Applied Artificial Intelligence, 32(1), 48–64. https://doi.org/10.1080/08839514.2018.1451032 \n",
    "\n",
    "What is auditing? ASQ. (n.d.). Retrieved December 9, 2021, from https://asq.org/quality-resources/auditing. \n",
    "\n",
    "Huang, S.-M., Yen, D. C., Yang, L.-W., & Hua, J.-S. (2008). An investigation of Zipf's law for fraud detection (DSS#06-10-1826R(2)). Decision Support Systems, 46(1), 70–83. https://doi.org/10.1016/j.dss.2008.05.003 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
